{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhSa9jfwHGoZ5u1Lp5Zw0p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suelen-prs/EBAC/blob/main/Exercicio2__Modulo24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importações necessárias\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import fetch_california_housing"
      ],
      "metadata": {
        "id": "26HVBz5VWoK7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Cite 5 diferenças entre o AdaBoost e o GBM.\n"
      ],
      "metadata": {
        "id": "X69ugGhaVX3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Abordagem de Ponderação:***\n",
        "\n",
        "**AdaBoost:** Aumenta o peso das instâncias mal classificadas pelos modelos anteriores, fazendo com que o modelo subsequente se concentre mais nessas instâncias.\n",
        "\n",
        "**GBM:** Usa o gradiente do erro de perda para guiar o processo de boosting, focando em reduzir o erro máximo ao invés de corrigir classificações incorretas.\n",
        "\n",
        "***Função de Perda:***\n",
        "\n",
        "**AdaBoost:** Usa uma função de perda exponencial por padrão.\n",
        "\n",
        "**GBM:** É flexível e pode utilizar várias funções de perda, tornando-o adequado para uma ampla gama de problemas, incluindo regressão e classificação.\n",
        "\n",
        "***Complexidade do Modelo Base:***\n",
        "\n",
        "**AdaBoost:** Tipicamente usa árvores de decisão de um único nível (stumps) como modelos base.\n",
        "**GBM:** Pode usar árvores de decisão mais profundas, permitindo uma modelagem mais complexa dos dados.\n",
        "\n",
        "***Regularização:***\n",
        "\n",
        "**AdaBoost:** Tem poucas opções de regularização inerentes ao algoritmo.\n",
        "\n",
        "**GBM:** Inclui várias opções de regularização, como o shrinkage (taxa de aprendizagem) e a poda das árvores, o que pode ajudar a prevenir overfitting.\n",
        "\n",
        "***Sensibilidade a Dados Ruidosos e Outliers:***\n",
        "\n",
        "**AdaBoost:** É bastante sensível a dados ruidosos e outliers devido à sua função de perda exponencial.\n",
        "\n",
        "**GBM:** Dependendo da função de perda escolhida, pode ser mais robusto a outliers."
      ],
      "metadata": {
        "id": "r5d6o7MFe_wJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Acesse o link Scikit-learn – GBM, leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM.\n"
      ],
      "metadata": {
        "id": "l6G4QiydVacq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# classificação com GBM\n",
        "\n",
        "# Carregar dados\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Dividir os dados\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Instanciar e treinar o modelo\n",
        "gb_clf = GradientBoostingClassifier(random_state=42)\n",
        "gb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Avaliar o modelo\n",
        "print(\"Classificação - Score no conjunto de teste:\", gb_clf.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dzUsrQ-gpQP",
        "outputId": "8f62078f-80a0-4f8a-fa8e-e403db0b1a2c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classificação - Score no conjunto de teste: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# regressão com GBM\n",
        "\n",
        "# Carregar o conjunto de dados de habitação da Califórnia\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Dividir os dados em conjuntos de treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Instanciar e treinar o modelo Gradient Boosting Regressor\n",
        "gb_reg = GradientBoostingRegressor(random_state=42)\n",
        "gb_reg.fit(X_train, y_train)\n",
        "\n",
        "# Avaliar o modelo no conjunto de teste\n",
        "score = gb_reg.score(X_test, y_test)\n",
        "print(\"Regressão - Score no conjunto de teste:\", score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mLX6Lqxg7en",
        "outputId": "a50b4124-8e30-4760-8477-a373d42b9970"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regressão - Score no conjunto de teste: 0.7756446042829697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Cite 5 Hyperparametros importantes no GBM.\n"
      ],
      "metadata": {
        "id": "OdnSqZuxVdmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**n_estimators:** Número de árvores de boosting a serem criadas.\n",
        "\n",
        "**learning_rate:** Taxa de aprendizagem, que encolhe a contribuição de cada árvore. Há um trade-off entre learning_rate e n_estimators.\n",
        "\n",
        "**max_depth:** Profundidade máxima das árvores de regressão. Controla a complexidade do modelo.\n",
        "\n",
        "**min_samples_split:** Número mínimo de amostras necessárias para dividir um nó interno.\n",
        "\n",
        "**loss:** A função de perda a ser otimizada. Varia dependendo do tipo de problema (por exemplo, regressão vs. classificação)."
      ],
      "metadata": {
        "id": "hp5cMFC2hL4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. (Opcional) Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo"
      ],
      "metadata": {
        "id": "ysA5sTiFVg8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o conjunto de dados e dividir\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Configurar o espaço de hiperparâmetros para testar\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.5],\n",
        "    'max_depth': [3, 4, 5]\n",
        "}\n",
        "\n",
        "# Instanciar o GridSearchCV com um GradientBoostingRegressor\n",
        "grid_search = GridSearchCV(GradientBoostingRegressor(random_state=42), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Executar o GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Melhores parâmetros e score\n",
        "print(\"Melhores Parâmetros:\", grid_search.best_params_)\n",
        "print(\"Melhor Score de Cross-Validation (neg_mean_squared_error):\", grid_search.best_score_)\n",
        "\n",
        "# Avaliar no conjunto de teste usando os melhores parâmetros\n",
        "best_model = grid_search.best_estimator_\n",
        "test_score = best_model.score(X_test, y_test)\n",
        "print(\"Score no Conjunto de Teste (R^2):\", test_score)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYNyEPdOha5n",
        "outputId": "c111b428-ddb8-432a-ee0a-8352f950011a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhores Parâmetros: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
            "Melhor Score de Cross-Validation (neg_mean_squared_error): -0.22553386249746515\n",
            "Score no Conjunto de Teste (R^2): 0.8288266356513045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Acessando o artigo do Jerome Friedman (Stochastic) e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos?"
      ],
      "metadata": {
        "id": "wrV9AZnpdUYJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A maior diferença entre o Gradient Boosting Machine (GBM) tradicional e o Stochastic Gradient Boosting Machine (SGBM), conforme introduzido por Jerome Friedman, reside no processo de amostragem utilizado durante o treinamento das árvores.\n",
        "\n",
        "No GBM tradicional, todas as árvores são treinadas usando todo o conjunto de dados de treinamento. Cada árvore tenta corrigir os erros da árvore anterior, focando nas instâncias mais difíceis de prever corretamente.\n",
        "\n",
        "No Stochastic Gradient Boosting Machine (SGBM), por outro lado, cada árvore é treinada em uma amostra aleatória do conjunto de dados de treinamento. Esta amostra é tirada com reposição (bootstrap sample), semelhante ao que é feito no Bagging. A principal diferença é que, em vez de construir árvores em paralelo como no Bagging, o SGBM constrói as árvores sequencialmente, onde cada nova árvore é construída para corrigir os erros deixados pelas árvores anteriores.\n",
        "\n",
        "Essa abordagem estocástica introduz mais diversidade nas árvores construídas, o que pode ajudar a reduzir o risco de overfitting e melhorar a generalização do modelo em dados não vistos. Além disso, treinar em subconjuntos dos dados torna o processo mais eficiente computacionalmente, permitindo que o SGBM lidere com grandes conjuntos de dados de forma mais eficaz."
      ],
      "metadata": {
        "id": "SJ4TyTlmdfx1"
      }
    }
  ]
}